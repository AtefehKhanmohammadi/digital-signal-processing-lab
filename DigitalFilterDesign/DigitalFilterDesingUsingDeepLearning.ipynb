{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Filter Design using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see digital filters as some block box which we should find the parameters of it. In this notebook, we will use deep learning to find the parameters of a digital filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra Review\n",
    "First I will review some linear algebra concepts which we will use in this notebook.\n",
    "In linear algebra we have different notations for vectors and matrices.\n",
    "### Notations\n",
    "In this notebook, I will use the following notations:\n",
    "\n",
    "* $x$: element of a vector(scalar)\n",
    "* $\\mathbf{x}$: The vector\n",
    "* $\\mathbf{X}$: The matrix\n",
    "* $\\mathcal{X}$: The set which contains the elements of the vector or matrix and is a subset of $\\mathbb{R}^n$ or $\\mathbb{C}^n$\n",
    "\n",
    "### Product of a vector and a matrix\n",
    "If the vector $\\mathbf{x}$ has $m_1$ elements, and the matrix $\\mathbf{W}$ has $m_1$ rows and $m_2$ columns, we say that $\\mathbf{x} \\in \\mathbb{R}^{m_1}$ and $\\mathbf{W} \\in \\mathbb{R}^{m_1 \\times m_2}$. the production of this vector and this matrix is defined as below:\n",
    "$$ \\mathbf{x_{1\\times m_1}} \\mathbf{M_{m_1 \\times m_2}} = \\mathbf{y_{1\\times m_2}}$$\n",
    "where $\\mathbf{y} \\in \\mathbb{R}^{m_2}$.\n",
    "\n",
    "### Matrix as a set of vectors\n",
    "If we have $n$ vectors $\\mathbf{x}_i \\in \\mathbb{R}^{m_1}$, we can define a matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times m_1}$ as below:\n",
    "$$ \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\vdots \\\\ \\mathbf{x}_n \\end{bmatrix}$$\n",
    "\n",
    "The transpose of a matrix $\\mathbf{X}$ is defined as below:\n",
    "$$ \\mathbf{X}^T = \\begin{bmatrix} \\mathbf{x}_1^T & \\mathbf{x}_2^T & \\cdots & \\mathbf{x}_n^T \\end{bmatrix}$$\n",
    "\n",
    "The inverse of a matrix $\\mathbf{X}$ is defined as below:\n",
    "$$ \\mathbf{X}^{-1} \\mathbf{X} = \\mathbf{I}$$\n",
    "\n",
    "We call $\\mathbf{X}$ a 2D tensor, and $\\mathbf{x}$ a 1D tensor.\n",
    "\n",
    "### Series of input signals\n",
    "We can consider each vector as a input signal to a digital filter. The output of the filter is the production of the input signal and the filter parameters. We can consider the filter parameters as a matrix. So, the output of the filter is a vector.\n",
    "the filter should behave correctly for all the input signals. So, we should tune the weights of the filter to have the correct output for all the input signals.\n",
    "\n",
    "We assumed that the number of input signals is $n$ and the number of elements of each input signal is $m_1$. So, the input matrix is $\\mathbf{X} \\in \\mathbb{R}^{n \\times m_1}$. If we give all input signals to the system the output matrix is calculated as below:\n",
    "$$ \\mathbf{Y_{n \\times m_2}} = \\mathbf{X_{n \\times m_1}} . \\mathbf{W_{m_1 \\times m_2}}$$\n",
    "\n",
    "In our case the output matrix should have the same size as the input matrix. So, $m_2 = m_1$. So, the output matrix is $\\mathbf{Y_{n \\times m_1}}$. We can calculate the output matrix as below:\n",
    "$$ \\mathbf{Y_{n \\times m_1}} = \\mathbf{X_{n \\times m_1}} . \\mathbf{W_{m_1 \\times m_1}}$$\n",
    "\n",
    "### Non-linearities\n",
    "To model a digital filter we cannot use a simple matrix multiplication. We should add some non-linearities to the system. So, we can use the following equation to model a digital filter:\n",
    "$$ \\mathbf{Y_{n \\times m_1}} = \\sigma(\\mathbf{X_{n \\times m_1}} . \\mathbf{W_{m_1 \\times m_1}})$$\n",
    "where $\\sigma$ is a non-linear function.\n",
    "\n",
    "Still, we cannot use this equation to model a digital filter. Because the relation between the input and output of a digital filter is not a simple matrix multiplication which is a linear relation.\n",
    "To illustrate this, we can consider $\\mathbf{y_{new}} = \\sigma^{-1}(\\mathbf{y})$.\n",
    "The relation between $\\mathbf{X}$ and $\\mathbf{y_{new}}$ is a linear relation. So, we didn't learn a non-linear relation between $\\mathbf{X}$ and $\\mathbf{y}$.\n",
    "\n",
    "The input signals have been related to the system weights using a linear matrix multiplication. But, the output of the system is not a linear function of the system weights. So, we should insert some non-linearities between the system weights and the output of the system. So, we can use the following equation to model a digital filter:\n",
    "$$ \\mathbf{Y_{n \\times m_1}} = \\sigma(\\sigma(\\mathbf{X_{n \\times m_1}} . \\mathbf{W_1}_{m_1 \\times m_2}) . \\mathbf{W_2}_{m_2 \\times m_1})$$\n",
    "where $\\mathbf{W_1}$ and $\\mathbf{W_2}$ are the system weights and $\\sigma$ is a non-linear function.\n",
    "\n",
    "This is a simple neural network with one hidden layer. We can add more hidden layers to the system to have a more complex system. We should tune the system weights to have the correct output for all the input signals.\n",
    "The general equation for a neural network with $L$ hidden layers is as below:\n",
    "$$ \\mathbf{Y_{n \\times m_1}} = \\sigma(\\sigma(\\cdots \\sigma(\\mathbf{X_{n \\times m_1}} . \\mathbf{W_1}_{m_1 \\times m_2}) . \\mathbf{W_2}_{m_2 \\times m_3}) \\cdots . \\mathbf{W_L}_{m_L \\times m_1})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "In practice, we cannot tune the system weights to have the correct output for all the input signals. So, we should find the system weights which have the minimum error for all the input signals. So, we should define a cost function to calculate the error of the system for all the input signals. We can use the following equation to calculate the cost function:\n",
    "$$ \\mathbf{J} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{Y_i} - \\mathbf{\\hat{Y}_i}$$\n",
    "where $\\mathbf{Y_i}$ is the output of the system for the $i$th input signal and $\\mathbf{\\hat{Y}_i}$ is the correct output of the system for the $i$-th input signal which we expect from the system.\n",
    "This cost function cannot be used because the negative values of the error can cancel the positive values of the error. So, we should use absolute value of the error to calculate the cost function. So, we can use the following equation to calculate the cost function:\n",
    "$$ \\mathbf{J} = \\frac{1}{n} \\sum_{i=1}^{n} |\\mathbf{Y_i} - \\mathbf{\\hat{Y}_i}|$$\n",
    "We call this cost function mean absolute error (MAE).\n",
    "This cost function is not differentiable for values of $\\mathbf{Y_i} = \\mathbf{\\hat{Y}_i}$. So, we should use a differentiable function to calculate the cost function. We can use the following equation to calculate the cost function:\n",
    "$$ \\mathbf{J} = \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{Y_i} - \\mathbf{\\hat{Y}_i})^2$$\n",
    "This cost function is called mean squared error (MSE).\n",
    "\n",
    "There are lots of other cost functions which can be used to calculate the error of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "We want to find the system weights which minimize the cost function. We can use gradient descent to find the extrema of the cost function.\n",
    "* Some of the cost functions haven't minimum. So, we should prove that the cost function has a minimum. This is a hard problem. We should prove that the cost function has a minimum for all the possible input signals.\n",
    "* Another problem is that the cost function can have local minimums. So, we should consider this problem in our algorithm and prevent the algorithm to be stuck in a local minimum. We use methods like stochastic gradient descent and mini-batch gradient descent to prevent the algorithm to be stuck in a local minimum.\n",
    "Now, we should find the gradient of the cost function. We can use the following equation to calculate the gradient of the cost function:\n",
    "$$ \\nabla \\mathbf{J} = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla (\\mathbf{Y_i} - \\mathbf{\\hat{Y}_i})^2$$\n",
    "In the right side of the equation, we have a summation of the gradients of the cost function for all the input signals. So, we can use the following equation to calculate the gradient of the cost function:\n",
    "$$ \\nabla \\mathbf{J} = \\frac{1}{n} \\sum_{i=1}^{n} 2(\\mathbf{Y_i} - \\mathbf{\\hat{Y}_i}) \\nabla (\\mathbf{Y_i} - \\mathbf{\\hat{Y}_i})$$\n",
    "In the gradient part, the only parameter which is related to the weights of the system is $\\mathbf{Y_i}$. So, we can use the following equation to calculate the gradient of the cost function:\n",
    "$$ \\nabla \\mathbf{J} = \\frac{2}{n} \\sum_{i=1}^{n} (\\mathbf{Y_i} - \\mathbf{\\hat{Y}_i}) \\nabla \\mathbf{Y_i}$$\n",
    "To optimize the system weights, we should update the system weights using the following equation:\n",
    "$$ \\mathbf{W} = \\mathbf{W} - \\alpha \\nabla \\mathbf{J}$$\n",
    "where $\\alpha$ is the learning rate.\n",
    "The learning rate is a hyperparameter of the system. We should tune the learning rate to have a good performance for the system.\n",
    "The dimension of the gradient of the cost function is the same as the dimension of the system weights. So, we can use the following equation to update the system weights:\n",
    "$$ \\mathbf{W} = \\mathbf{W} - \\alpha \\frac{2}{n} \\sum_{i=1}^{n} (\\mathbf{Y_i} - \\mathbf{\\hat{Y}_i}) \\nabla \\mathbf{Y_i}$$\n",
    "There isn't just one weight matrix in the system. We have different weight matrices in the system. So, we should update all the weight matrices using the following equation:\n",
    "$$ \\mathbf{W_i} = \\mathbf{W_i} - \\alpha \\frac{2}{n} \\sum_{k=1}^{n} (\\mathbf{Y_k} - \\mathbf{\\hat{Y}_k}) {\\nabla}_{\\mathbf{W_i}} \\mathbf{Y_k}$$\n",
    "where $i$ is the index of the weight matrix.\n",
    "The term ${\\nabla}_{\\mathbf{W_i}} \\mathbf{Y_k}$ is the gradient of the output of the system with respect to the system weights. The dimension of the gradient of the output of the system with respect to the system weights is the same as the dimension of the system weights. Because calculate the gradient of the output of the system with respect to the system weights, we should calculate the gradient of the output of the system with respect to each element of the system weights. So, the dimension of the gradient of the output of the system with respect to the system weights is the same as the dimension of the system weights.\n",
    "For an arbitrary loss function $\\mathbf{J}$, we can use the chain rule to calculate the gradient of the output of the system with respect to the system weights. So, we can use the following equation to calculate the gradient of the loss function with respect to the system weights:\n",
    "$$ {\\nabla}_{\\mathbf{W_i}} \\mathbf{J} = \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{Y}} {\\nabla}_{\\mathbf{W_i}} \\mathbf{Y}$$\n",
    "where $\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{Y}}$ is the gradient of the loss function with respect to the output of the system and ${\\nabla}_{\\mathbf{W_i}} \\mathbf{Y}$ is the gradient of the output of the system with respect to the system weights.\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "As I mentioned earlier, to prevent the algorithm to be stuck in a local minimum, we should use methods like stochastic gradient descent and mini-batch gradient descent. In stochastic gradient descent, we use just one input signal to update the system weights. So, we can use the following equation to update the system weights:\n",
    "$$ \\mathbf{W_i} = \\mathbf{W_i} - \\alpha \\frac{2}{n} (\\mathbf{Y} - \\mathbf{\\hat{Y}}) {\\nabla}_{\\mathbf{W_i}} \\mathbf{Y}$$\n",
    "where $\\mathbf{Y}$ is the output of the system for the input signal and $\\mathbf{\\hat{Y}}$ is the correct output of the system for the input signal which we expect from the system. We choose the input signal randomly.\n",
    "### Mini-batch Gradient Descent\n",
    "In mini-batch gradient descent, we use a batch of input signals to update the system weights. So, we can use the following equation to update the system weights:\n",
    "$$ \\mathbf{W_i} = \\mathbf{W_i} - \\alpha \\frac{2}{n} \\sum_{k=1}^{b} (\\mathbf{Y_k} - \\mathbf{\\hat{Y}_k}) {\\nabla}_{\\mathbf{W_i}} \\mathbf{Y_k}$$\n",
    "where $b$ is the batch size and $b \\leq n$.\n",
    "Mini-batch gradient descent is a trade-off between stochastic gradient descent and batch gradient descent. In stochastic gradient descent, we use just one input signal to update the system weights. So, we have a noisy gradient. In batch gradient descent, we use all the input signals to update the system weights. So, we have a smooth gradient. In mini-batch gradient descent, we use a batch of input signals to update the system weights. So, we have a gradient which is smoother than stochastic gradient descent and noisier than batch gradient descent.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
